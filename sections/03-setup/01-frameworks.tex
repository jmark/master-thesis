\subsection{Computational Frameworks}
\label{sec:computational-frameworks}
The analyzed numerical solvers are part of the simulation frameworks, FLASH and
FLEXI. Since both frameworks have been modified, a complete set of source code
is provided, available on the attached DVD. Any changes on the source code were
logged by the version control system \emph{git} (https://git-scm.com).

\subsubsection*{FLASH}
The project page (http://flash.uchicago.edu/site/flashcode/, \today) says:
\begin{quote}
The FLASH code, currently in its 4th version, is a publicly available high
performance application code which has evolved into a modular, extensible
software system from a collection of unconnected legacy codes. FLASH consists
of inter-operable modules that can be combined to generate different
applications. The FLASH architecture allows arbitrarily many alternative
implementations of its components to co-exist and interchange with each other.
A simple and elegant mechanism exists for customization of code functionality
without the need to modify the core implementation of the source. A built-in
unit test framework combined with regression tests that run nightly on multiple
platforms verify the code. 
\end{quote}

The framework is widely adopted within the astrophysics community and serves as
reference for testing and comparing the Galerkin schemes provided by FLEXI.
Following hydrodynamics solvers came into use: PPM (cf.
\cite{colella1984piecewise}), Bouchut 3 and Bouchut 5 (cf.
\cite{bouchut2007multiwave,bouchut2010multiwave}). Theses solvers are 
specifically designed for supersonic turbulence simulations.

\subsubsection*{FLEXI}
The project page (http//www.flexi-project.org, \today) says:
\begin{quote}
Flexi is developed by the team of the Numerics Research Group hosted at the
Institute of Aero- and Gasdynamics at the University of Stuttgart. We are
interested in efficient and robust numerical methods for applications in scale
resolving CFD and we apply these methods to a variety of large scale
physical and industrial problems.
\end{quote}

\paragraph{Modifications}
Flexi provides an implementation of discontinuous Galerkin methods and is
primarily focused on air flow applications in research and industries. Thus,
the simulation of isothermal supersonic turbulences was originally not
supported.  Following contributions to the source code had to be implemented
beforehand:
\begin{quote}
\begin{description}
    \item [polytropic cooling] (cf. \sec{governing-equations}/\emph{Equation of State})
    \item [bulk motion correction] (cf. \sec{governing-equations}/\emph{Bulk Motion})
    \item [turbulent forcing] (cf. \sec{turbuforcing})
    \item [shock capturing] (cf. \sec{shock-capturing})
\end{description}
\end{quote}
Additionally, analysis and data transfer tools were developed (cf. \sec{trafo}).

\paragraph{Riemann solver}
Flexi offers a number of approximate Riemann solvers. After many tests we
settled down to the HLLE (Harten-Luv-Lax Entropy-fix) (cf. \cite{balsara_2010})
flux for two reasons.  First, it is not far from the Bouchut solvers (FLASH) in
technical terms and it was the only one that did not crash. The latter is the
reason, a comparative study with different flux functions was not possible in this
thesis.

\paragraph{Time Integration}
Three different methods of numerical time integration came into use: Euler (FV),
Midpoint (MP) and third order Runge-Kutta (RK3) (cf. \sec{time-integration}).

\paragraph{Hybrid Scheme}
As pointed out in the shock discussion in \sec{theory-shocks}, pure Galerkin schemes
are not suitable for supersonic turbulence simulations. Therefore, a compromise
between FV and DG schemes is used (cf. \sec{shock-capturing}) where FV and DG
are mixed together to a hybrid scheme (HY). In order to counter-check this scheme,
the same simulations were also performed with FV only. See \sec{shock-capturing}.

\subsubsection*{ADDITIONAL TOOLS}
The simulations were run on \vip{CHEOPS} at the RRZK\footnotemark[1] (Univ. of Cologne)
and on \vip{SuperMUC}\footnotemark[2] at the \vip{Leibniz
Supercomputing Center}.

For analyzing and visualizing the data we used the \vip{Python} scripting
language with its large set of scientific libraries\footnotemark[3]: numpy, pandas, h5py, ipython
and matplotlib.

Batch processing of myriads of small scripts was made possible by \vip{GNU
Parallel} (cf. \cite{Tange2011a}).

The scripts and tools are also placed on the attached DVD.

\footnotetext[1]{http://rrzk.uni-koeln.de}
\footnotetext[2]{https://www.lrz.de/services/compute/supermuc/}
\footnotetext[3]{https://scipy.org}
